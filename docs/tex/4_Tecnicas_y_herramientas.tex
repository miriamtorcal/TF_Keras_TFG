\capitulo{4}{Técnicas y herramientas}

\section{Metodología}
A lo largo del proyecto se intentado seguir la \textit{metodología ágil Scrum}, pero adaptada ya que para pdoer aplciar esta metodología es necesario contar con un equipo, en el cuál los diferentes miembros se reparten las roles entre los diferentes miembros que lo conforman. En este caso, lso roles recaeen todo sobre una única persona.

\imagen{scrum}{Pasos de la metodología Scrum}

En primer lugar se encuentra el \textit{Product Backlog} \cite{scrum} que se trata del alcance del proyecto, el cuál va variando dependiendo de los \textit{feedbacks} que se van obteniendo en cada \textit{sprint}.

Seguidamente, se encuentra el \textit{Sprint Backlog}, dónde se marcan los requerimientos que deben de alcanzar durante el \textit{sprint} que se va a iniciar, es decir, se trata de acortar las tareas de cada uno 
de los \textit{sprints}.

La siguiente etapa es el \textit{Sprint}, en la cuál tiene lugar la planificación, la implementación, revisión y retrospectiva de la nueva característica software.
Esta etapa suele tener una duración de una a dos semanas.

Como último paso del proceso, se enecuentra el \textit{incremento del producto}, esta fase consiste en tener una reunión con el cliente con la nueva característica en funcionamiento con el objetivo de obtener una \textit{retroalimentación} por parte del cliente y así volver a empezar el proceso de nuevo.


\section{Lenguaje de programación}
A la hora de empezar un nuevo proyecto es importante relacionado con el \textit{Machine Learning} y el \textit{Edge Computing}, es muy importante seleccionar el lenguaje, con el cuál queremos trabajar destacando dos: \textbf{Python} \cite{python} y \textbf{Matlab} \cite{matlab}.
En este caso se decantó por el uso de \textit{Python}, debido al mayor conocimiento de este lenaguaje y haber trabajado más con este lenguaje que con \textit{Matlab}.
No hay grandes ventajas entre escoger uno u otro.


\section{Algoritmo de detección}
A parte de tener claro el lenguaje que se quiere utilizar, otra característica a tener en cuenta es elegir el \textit{algoritmo de detección} en el que se va a basar el modelo.
Existen diferentes algoritmos:
\begin{list}{\textbullet}{ %
    \addtolength{\itemsep}{-2mm} %
    \setlength{\itemindent}{2mm}}

    \item \textbf{CNN:} \textit{(Convolutional Neuronal Network)} es la opción maás básica que se puede escoger,ya que se parte de una red neuronal convolucional \cite{cnn} la cuál itera la imagen hasta devolver las posiciones de los objetos que detecta.
    
    Está opción trae consigo diferentes inconvenientes:
    \begin{list}{\textbullet}{ %
        \addtolength{\itemsep}{-2mm} %
        \setlength{\itemindent}{2mm}}
        \item Si la imagen detecta varios objetos, situados en zonas opuestas, ¿cuántos píxeles tendremos que desplazarnos en cada dirección?.
        \item El tiempo de cómputo es variable, pudiendo llegar a ser muy largo, ya que por cada movimiento implica una clasificación individual con la red.
        \item Deetectar un objeto dentro de la red, no indica que se poseen los valores 'x' e 'y' de su posición.
        \item Si por un casual el desplazamiento de píxeles que se realiza es muy pequeño, podríamos estar detectadndo el mismo objeto múltiples veces.
        \item Si dos objetos se encuentran muy juntos, se podrían llegara a detectar como un único objeto.    
    \end{list}
    \item \textbf{R-CNN:} \textit{(Region Based Convolutional Neural Networks)} surgen en el año 2014, con la siguiente propuesta: determianr primero las regiones de interés de la imagen y después realizar la clasificación de imagenes sobre dichas áreas usando una red preentrenada.\cite{r-cnn}
    Esto implica, que haya un primer algoritmo que detecte las áreas de interés de la imagen, las cuáles pueden ser muchas y de diversos tamaños. Seguidamente, se pasán las diferentes regiones por la CNN, validandose las clases correctas mediante un clasificador bianrio, de tal forma, que se eliminarán
    las que tenga un bajo nivel de confianza. Por último, se ajustaría la posición mediante un regresor.
    
    \imagen{r-cnn-regions}{Pasos de la detección en R-CNN}

    \clearpage

    \item \textbf{Fast R-CNN / Faster R-CNN:} Son dos algoritmos que surgen como mejora a R-CNN: \cite{faster_rcnn}
    \begin{list}{\textbullet}{ %
        \addtolength{\itemsep}{-2mm} %
        \setlength{\itemindent}{2mm}}
        \item \textbf{Fast R-CNN:} mejora el algoritmo inicial reutilizando algunso recursos, como las \textit{features} extraídaa por la CNN, de tal forma que se agiliza el entreno y la detección de las imágenes.
        Esta red, posee también mejoras en el cálculo del IoU \textit{(Intersection Over Union)} y en la función de \textit{Loss}. Pero a pesar de esto, no tiene mejoras drásticas en la velocidad de entrenamiento y en la detección.
        \imagen{fast-RCNN}{Arquitectura red Fast-RCNN}
       \item \textbf{Faster R-CNN:} logra una mejora de velocidad al integrar el algoritmo de \textit{region proposal} \cite{region_proposal} sobre la propia CNN.
       Además aperece el concepto de usar \textit{anchors} fijos, lo cuál consiste en usar tamaños pre calculados para la detección de obejtos de la red.
       \imagen{fasterRCNN}{Arquitectura red Faster-RCNN}
    \end{list}
    
    \clearpage

    \item \textbf{YOLO:} surge en 2016, su nombre viene formado por las siglas de \textit{You Only Look Once}.\cite{yolov4}
    Esta red, como su propio nombre indica hace una única pasada a la red y detecta todos los obejtos para los que ha sido entrenada para clasificar, al realizar un único vistazo obtiene velocidades muy buenas en equipos que no son necesariamente potentes. Lo cuál permite, detecciones en tiempo real de cientos obejtos de forma simúltanea y su ejecución en dispositivos móviles.  
\end{list}
Debido a esto, el modelo escogido ha sido YOLO y en su versión 4, la cuál fue lanzada en Abril del año 2020.

\imagen{resultsYOLOv4}{Comparativa resultados Dataset COCO}

\section{Tensorflow}
Tensorflow \cite{tensorflow} es una biblioteca de código abierto, la cuál fue lanzada en el año 2015 por Google, la cuál es muy utilizada por muchas empresas resultado de gran utilidad por su versatilidad y nivel de desarrollo.

Esta herramienta se basa en el \textbf{Deep Learning,} a pesar de que el mercado habia herramientas similares como \textit{DistBelief} \cite{distBelief}, la cuál también fue construida por Google en el año 2011, como un sistema propietario de aprendizaje automático. Su uso creció rápidamente debido al uso de compaañias como \textit{Alphabet}. Pero los años pasaron y el avance de la tecnología 
hizo que las necesidades aumentasen, haciendo que Google invirtiese tiempo para mejorarla, dando lugar a la actual \textbf{Tensorflow,} una herramienta con un conjunto de datos mayor y con mayor capacidad de almacenamiento y modificación.

Se basa en un sistema de redes neuronales, lo cuál permite relacionar varios datos en la red de manera simultánea, es decir, imita lo que hace el cerebro humano.

\section{TensorRT}
TensorRT \cite{tensorrt} es un \textit{framework} de aprendizaje automático, el cuál fue publicado por \textbf{Nvidia} para ejecutar inferencias que son interferencias de aprendizaje automático en su hardware. Este \textit{framework}, se encuentra altamente cualificado para ejecutarse en \textit{GPUs Nvidia}, siendo una de las formas más rápidas de ejecutar un modelo en este momento.